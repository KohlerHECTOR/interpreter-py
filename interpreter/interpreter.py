from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.utils import check_for_correct_spaces
from stable_baselines3.common.monitor import Monitor

from .policies import DTPolicy, SB3Policy, ObliqueDTPolicy

import numpy as np
from copy import deepcopy
from operator import itemgetter


class Interpreter:
    """
    A class to interpret a neural net policy using a decision tree policy.
    It follows algorithm 1 from https://arxiv.org/abs/2405.14956

    Parameters
    ----------
    oracle : object
        The oracle model that generates the data for training.
        Usually a stable-baselines3 model from the hugging face hub.
    tree_policy : object
        The decision tree policy to be trained.
    env : object
        The environment in which the policies are evaluated (gym.Env).
    data_per_iter : int, optional
        The number of data points to generate per iteration (default is 5000).

    Attributes
    ----------
    oracle : object
        The oracle model that generates the data for training.
    tree_policy : object
        The decision tree policy to be trained.
    data_per_iter : int
        The number of data points to generate per iteration.
    env : object
        The monitored environment in which the policies are evaluated.
    tree_policies : list
        A list to store the trained tree policies over iterations.
    tree_policies_rewards : list
        A list to store the rewards of the trained tree policies over iterations.
    """

    def __init__(self, oracle, tree_policy, env, data_per_iter=5000):
        assert isinstance(oracle, SB3Policy) and (
            isinstance(tree_policy, DTPolicy)
            or isinstance(tree_policy, ObliqueDTPolicy)
        )
        self.oracle = oracle
        self.tree_policy = tree_policy
        self.data_per_iter = data_per_iter
        self.env = Monitor(env)
        check_for_correct_spaces(
            env, self.tree_policy.observation_space, self.tree_policy.action_space
        )
        check_for_correct_spaces(
            env, self.oracle.observation_space, self.oracle.action_space
        )

    def train(self, nb_iter):
        """
        Train the decision tree policy using data generated by the oracle.

        Parameters
        ----------
        nb_iter : int
            The number of training iterations.
        """
        print("Fitting tree nb {} ...".format(0))
        S, A = self.oracle.generate_data(self.env, self.data_per_iter)
        self.tree_policy.fit_tree(S, A)
        tree_reward, _ = evaluate_policy(self.tree_policy, self.env)

        self.tree_policies = [deepcopy(self.tree_policy)]
        self.tree_policies_rewards = [tree_reward]

        for t in range(nb_iter - 1):
            print("Fitting tree nb {} ...".format(t + 1))
            S_new, A_new = self.tree_policy.generate_data(self.env, self.data_per_iter)
            S = np.concatenate((S, S_new))
            A = np.concatenate((A, self.oracle.predict(S_new)[0]))

            self.tree_policy.fit_tree(S, A)
            tree_reward, _ = evaluate_policy(self.tree_policy, self.env)
            print("Tree policy reward: {}".format(tree_reward))

            self.tree_policies += [deepcopy(self.tree_policy)]
            self.tree_policies_rewards += [tree_reward]

    def get_best_tree_policy(self):
        """
        Get the best decision tree policy based on the rewards.

        Returns
        -------
        best_tree_policy : object
            The best decision tree policy.
        best_reward : float
            The reward of the best decision tree policy.
        """
        index, element = max(enumerate(self.tree_policies_rewards), key=itemgetter(1))
        return self.tree_policies[index], element
